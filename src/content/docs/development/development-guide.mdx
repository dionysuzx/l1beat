---
title: Development guide
description: Development guide for Kittynode.
---

import { Steps } from '@astrojs/starlight/components';

Kittynode is open to everyone, and happily welcomes any contributions!

## Working with the codebase

Get started with the codebase and running development commands.

### Prerequisites

<Steps>

1. Install [Rust](https://www.rust-lang.org/tools/install):

    ```bash
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
    ```

2. Install [just](https://github.com/casey/just):

    ```bash
    cargo install just
    ```

3. Install [Node.js](https://nodejs.org/en/download/package-manager).

4. Install [pnpm](https://pnpm.io/installation).

</Steps>

### Development commands

<Steps>

1. Clone the repo and change into the directory:

    ```bash
    git clone git@github.com:kittynode/kittynode.git && cd kittynode
    ```

2. Install dependencies:

    ```bash
    pnpm install
    ```

3. Install Rust dev tools:

    ```bash
    just install-dev-tools
    ```

4. See the full list of commands:

    ```bash
    just -l
    ```

</Steps>

## Managing releases

We push a tag `<package-name>-0.y.z-alpha` to GitHub, and CI will build and publish a draft release for the package. We auto-generate a changelog with GitHub, and publish a latest release (this gives us links to refer to the latest release).

You can find all the latest releases [here](https://github.com/kittynode/kittynode/releases).

We will support package-level changelogs and semantic versioning in the future, but during the rapid development phase we don't want to waste time on that.

## Coding principles

### Testing philosophy

Here are the tests we (currently) employ in the project:

- Unit tests for kittynode-core
- Behavior tests for kittynode-cli

Inside of unit tests we discourage mocking. For example, you could mock out the filesystem and networking like Docker (things on the host OS), but this leaks the integration boundary into the data transformation logic in the unit tests. A unit test is basically just taking in some primitive data, transforming it, and getting something out. This is what we strive to capture in the unit tests, and leave the interface boundaries for higher level tests (like behavior tests).

We also remain generally cognizant of the testing pyramid (we want lots of cheap unit tests, with less expensive integration tests). This should form a "sufficient bug catching filter" (if you view it as an inverted pyramid).

Additionally, we don't strive for some kind of 100% code coverage. That's not the goal of writing tests in this project. We try to colocate our tests to explain how the codebase works for developers, but also ensure that our function behaves in the way we expect it to.

### Code coverage

We include coverage from the integration tests in our total code coverage. This means code coverage is a holistic quality check for the codebase that represents an aggregate of our testing strategies and the code paths we are hitting in our test suite.

It would be useful to have a sense of what percentage of code paths are hit by unit tests versus integration tests, but we don't do that (for now).

### Logging

We like logs. In library functions it's essential, in "binding" layers it is highly encouraged (as the API is homogeneous and we should trace out the entry here), and in other layers it is optional / as needed. In the other layers it's best to use your judgement but not litter with too many logs.

In consumers of library functions we (optionally) log at the entrypoint. Inside the library functions we log after the success / failure of the operation. We also always write our logs in sentence case without an ending period.

In JavaScript we should at minimum log errors when calling library functions or external APIs at all (in a try-catch), and other logging is (currently) just best judgement.

The log functions we use are:

- Rust: `info!()` and `error!()`
- JavaScript: `console.info()` and custom `error()`

## Troubleshooting

### Deploying to iOS

Some notes until we get it resolved upstream in Tauri:

1. Make sure the physical device is properly connected and xcode is open.
2. Make sure any last build is uninstalled from the device.
3. See the justfile for more clean up commands.
4. If you don't see the iOS IP, then it's a connection issue.

### Other

You can also try some of these commands which used to be in the `justfile`:

```
# clean the ios app
ios-clean:
  rm -rf ~/Library/Developer/Xcode/DerivedData
  rm -rf ~/Library/Developer/Xcode/Archives
  rm -rf ~/Library/Developer/Xcode/Projects

# reset ios simulators
ios-erase:
  xcrun simctl erase all
```
